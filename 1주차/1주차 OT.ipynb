{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56bea89e",
   "metadata": {},
   "source": [
    "- 교재: 딥러닝 파이토치 교과서\n",
    "\n",
    "https://github.com/gilbutITbook/080289\n",
    "\n",
    "https://product.kyobobook.co.kr/detail/S000001834807\n",
    "\n",
    "https://github.com/aninsung\n",
    "\n",
    "- 책을 구매 하는 것을 지향\n",
    "- 파이토치 사용으로 경향 변화\n",
    "\n",
    "- 실습으로 평가 지향\n",
    "- 깃허브에 잘 정리\n",
    "- colab도 미리 준비 하는 것 지향\n",
    "- 수업 분위기 방해하는 행위 지양\n",
    "- 지각 X\n",
    "\n",
    "# OT 요약 — 주요 내용\n",
    "\n",
    "(2025-09-02 화 / 김승찬님 노트 기반)\n",
    "\n",
    "---\n",
    "\n",
    "## 한눈에 요약\n",
    "\n",
    "교수님이 수업(실습 중심 AI/기계학습) 운영 방식, 교재 선택 이유, 과제 제출 방식(GitHub/Colab 중심), 평가 기준, 출결/수업 예절 등에 대해 상세히 안내하셨습니다. 실습·과제 정리가 성적과 포트폴리오에 결정적으로 중요하니 **GitHub에 주차별 폴더를 만들어 정리하고, Colab 준비를 미리 해두라**는 점이 핵심입니다. 또한 수업 참여(지각·수업 방해 등)에 대한 엄격한 기대가 분명히 제시되었습니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 수업/교과 구성\n",
    "\n",
    "- 수업 성격: **실습 중심의 AI(기계학습) 강의** — 이론도 다루지만 실습(프로그램)이 핵심.\n",
    "- 전체 커리큘럼(언급): 개론, 기계학습, EDA(탐색적 데이터분석), 프로그램(실습) 등으로 구성. (교수님은 실습 비중이 큰 편이라 설명)\n",
    "- **교재**: 기본 교재로 **딥러닝(파이토치) 교과서** 권장.\n",
    "    - 이유: 최신 학계·컨퍼런스 코드는 PyTorch 기반이 많고 실무 트렌드도 PyTorch 쪽으로 이동.\n",
    "    - 단, **TensorFlow 교재**로 대체 가능(원하면 텐서플로우 책을 사서 사용해도 됨). 교수님은 PyTorch를 추천하지만 언어 강제는 아님.\n",
    "- 오전/오후반 차이: 오전반은 TensorFlow 중심(교재 코드 차이만), 내용은 동일.\n",
    "\n",
    "---\n",
    "\n",
    "## 실습·과제 제출 방식\n",
    "\n",
    "- **주차별 폴더 구조**로 과제·데이터·코드 모두 정리해서 **GitHub**에 올릴 것.\n",
    "    - 교수님이 GitHub(깃허브) 업로드를 **수시로 체크**함.\n",
    "    - 제출 시점: 해당 주 수업 전까지(교수님 안내문 기준으로 주차별로 정해진 기한 엄수).\n",
    "- **Colab 준비**: 이번 학기에는 Colab으로 실습 진행 예정이므로 미리 Colab 환경을 세팅해 둘 것.\n",
    "- 과제·실습 형식: 교수님이 제공하는 소스 기반 설명을 넘어, 학생이 스스로 코드를 구현·확장하는 형태(교수님은 일부 더 어려운 과제 제시).\n",
    "- **GitHub 점수 비중 큼**: 깃허브(레포 정리)가 **성적에 결정적 영향(예: 30점 배정)** — 포트폴리오 용도로도 중요.\n",
    "\n",
    "---\n",
    "\n",
    "## 평가 방식(교수님 안내대로 요약)\n",
    "\n",
    "- 출석: **10점**\n",
    "- 중간/기말(언급된 항목 포함): 각각 대략 10점 내외(교수님이 중간·기말 시험 비중 언급).\n",
    "- **깃허브(실습·코드 관리)**: **30점**(중요) — 잘 정리하면 C 이상 보장, 미흡하면 F 가능성 있음.\n",
    "- 총평: 실습·깃허브 정리가 성적의 핵심이며, 단순 시험만으로는 높은 성적 보장 어려움.\n",
    "\n",
    "---\n",
    "\n",
    "## 출결·수업예절·교수님 기대\n",
    "\n",
    "- **출석/지각**\n",
    "    - 출석 중요(출석 10점). 지각·무단결석은 수업 방해로 매우 싫어하심.\n",
    "    - 불가피한 사유(병원 등)는 증빙(예: 약 봉지 사진)으로 인정 가능.\n",
    "- **수업 태도**\n",
    "    - 수업 중 **카톡·잡담·수업 방해 행동 금지**(특히 옆에서 수업 방해하면 강하게 지적).\n",
    "    - 너무 졸거나 엎드려 자는 등 수업 분위기 저해 행위 자제. 필요하면 잠깐 나갔다 다시 들어오라 하심(수업 방해보다 낫다).\n",
    "- **교수님 교육 철학**\n",
    "    - 학생들이 “한 학기 끝나고 AI에 도움이 되었다”고 느끼게 하고 싶어 하심 — 그래서 열정적으로 강의하고 개선하려는 태도 표출.\n",
    "    - 학생의 노력(깃허브 정리 등)에 따라 성취가 달라진다고 강조.\n",
    "\n",
    "---\n",
    "\n",
    "## 실무적 지시 / 학생이 즉시 해야 할 것\n",
    "\n",
    "1. **GitHub 레포 준비**: 강의용 레포 만들고 주차별 폴더 구조로 정리 시작. (교수님에게 레포 주소/학번/이름을 이메일로 제출 요구)\n",
    "2. **Colab 세팅**: Colab 계정(구글 계정) 준비, 필요한 라이브러리 설치 연습.\n",
    "3. **교재 구입/선택**: PyTorch 교재 권장 — 원하면 TensorFlow 교재 가능(언어 강제 X).\n",
    "4. **출결·지각 습관 정리**: 지각하지 않도록 이동 시간 확보, 정당한 사유 시 증빙 준비.\n",
    "5. **메일/홈페이지 확인**: 교수님 공지(강의계획서·과제)는 교수님 홈페이지/메일을 통해 공지되니 자주 확인.\n",
    "6. **수업 참여 태도**: 수업 중 집중, 질문은 적절히 하되 채팅 등으로 방해하지 않기.\n",
    "\n",
    "---\n",
    "\n",
    "## 교수님이 특별히 언급하신 포인트(요점)\n",
    "\n",
    "- **GitHub(깃허브)가 곧 성적 및 포트폴리오** — 잘 정리해두면 이후 취업·프로젝트에 큰 자산.\n",
    "- PyTorch 선택 이유: 최근 연구/컨퍼런스 소스 대부분 PyTorch 기반. 실무·연구 트렌드 반영.\n",
    "- 수업은 “실습을 많이 하는 기계학습 프로그램”으로 생각하라.\n",
    "- 수업 규모 문제: 인원 많은 경우(40명 이상) 운영 어려움 → 필요 시 반 편성(오전 이동 권장). (오전 이동자에겐 교수님이 비공식적 혜택 언급)\n",
    "\n",
    "---\n",
    "\n",
    "> AI란?\n",
    "> \n",
    "\n",
    "⇒ 인공적으로 인간이 만들어낸 지능.\n",
    "\n",
    "- 지능이란?\n",
    "    \n",
    "    \n",
    "\n",
    "- **인공지능 기초를 위한 FAQ** <- GPT 추출\n",
    "    - **인공지능에서 ‘지능’에 해당하는 기능은 무엇인가?**\n",
    "        \n",
    "        지능은 주로 **지각(perception)**, **학습(learning)**, **추론(reasoning)**, **계획(planning)**, **언어처리(language)**, **문제해결(problem solving)**, **적응(adaptation)** 등으로 나뉩니다. AI는 이 기능들을 알고리즘과 데이터로 근사해 특정 태스크(예: 이미지인식, 번역, 게임플레이 등)를 수행합니다.\n",
    "        \n",
    "    - **인공지능의 종류 3가지 — 지도학습, 반지도학습, 강화학습 설명**\n",
    "        - **지도학습(supervised learning)**: 입력과 정답(레이블)이 주어질 때 모델을 학습. 예: 이미지 → 클래스.\n",
    "        - **반지도학습(semi-supervised learning)**: 일부 데이터만 레이블이 있고 나머지는 레이블 없음. 레이블을 효율적으로 활용해 성능 향상.\n",
    "        - **강화학습(reinforcement learning)**: 에이전트가 환경과 상호작용하며 보상(reward)을 최대화하도록 행동을 학습. 예: 게임, 로봇 제어.\n",
    "    - **`전통적 프로그래밍 vs 인공지능 프로그램의 차이`**\n",
    "        - 전통적: 규칙(명시적 로직)을 사람이 작성 → 입력 → 출력.\n",
    "        - AI: 데이터와 목적함수(손실)를 주고, 모델(파라미터)을 학습시켜 입력→출력을 자동으로 일반화. 즉 `규칙을 **학습**`한다는 점이 핵심 차이.\n",
    "    - **`딥러닝과 머신러닝의 차이`**\n",
    "        - 머신러닝: SVM, 결정트리, 회귀 등 광범위한 알고리즘. 특징(피처)을 사람이 설계하는 경우가 많음.\n",
    "        - 딥러닝: 다층 신경망(특히 깊은 네트워크)을 사용해 원시 데이터로부터 특징을 자동 추출. 대량 데이터와 연산 자원이 필요.\n",
    "    - **Classification과 Regression의 주된 차이**\n",
    "        - **Classification**: 출력이 범주(이산값). 예: 고양이/개. 일반적 손실: 교차엔트로피.\n",
    "        - **Regression**: 출력이 연속값(숫자). 예: 주택가격 예측. 일반적 손실: MSE(평균제곱오차).\n",
    "    - **머신러닝에서 차원의 저주(curse of dimensionality)란?**\n",
    "        \n",
    "        차원이 늘어나면 데이터가 희소해지고(거리 기반 알고리즘의 신뢰성 저하), 모델이 과적합하기 쉬워지며 계산량이 급증하는 현상. 결국 샘플 수 대비 특징 수가 너무 많아 학습이 어려워짐.\n",
    "        \n",
    "    - **Dimensionality Reduction는 왜 필요한가?**\n",
    "        - 노이즈·중복 특징 제거 → 과적합 감소\n",
    "        - 계산·저장 비용 절감\n",
    "        - 시각화(2D/3D로 축소)\n",
    "        - 거리나 밀도 기반 기법의 성능 향상\n",
    "            \n",
    "            방법: PCA, LDA, t-SNE, UMAP, Autoencoder 등.\n",
    "            \n",
    "    - **Ridge와 Lasso의 공통점과 차이점 (Regularization, Scaling)**\n",
    "        - 공통점: 둘 다 과적합을 줄이기 위해 손실에 페널티(규제)를 추가. → 가중치 크기 제한.\n",
    "        - 차이점:\n",
    "            - **Ridge (L2)**: 페널티 = λ ∑w_i^2. 가중치를 연속적으로 작게 함(0으로 만들지 않음).\n",
    "            - **Lasso (L1)**: 페널티 = λ ∑|w_i|. 일부 가중치를 정확히 0으로 만들어 **특징 선택** 효과.\n",
    "        - **Scaling 중요**: 규제는 가중치 크기에 의존하므로 표준화(평균0, 분산1)가 필요.\n",
    "    - **Overfitting vs Underfitting**\n",
    "        - **Overfitting**: 학습데이터에는 잘 맞지만 검증/실제 데이터에 일반화 못함. 증상: 훈련 손실 낮고 검증 손실 높음. 해결: 규제, 데이터 증가, 단순화, early stopping.\n",
    "        - **Underfitting**: 모델이 데이터의 패턴을 충분히 학습하지 못함. 증상: 훈련/검증 모두 성능 낮음. 해결: 더 복잡한 모델, 더 적합한 특징, 학습시간 증가.\n",
    "    - **Feature Engineering과 Feature Selection의 차이**\n",
    "        - **Feature Engineering**: 새로운 특징(파생변수)을 만들거나 변환(예: 로그, 다항항)해 모델 성능을 높이는 작업.\n",
    "        - **Feature Selection**: 의미 있는 기존 특징만 골라 사용하는 작업(필요없는 변수 제거). 두 작업은 종종 함께 수행됨.\n",
    "    - **전처리(Preprocessing)의 목적과 방법? (노이즈, 이상치, 결측치)**\n",
    "        - 목적: 데이터 품질 개선 → 학습 안정성 및 정확도 향상.\n",
    "        - 방법: 결측치 처리(삭제/대체: 평균·중앙값·모델 기반), 이상치 탐지·처리(삭제, 클리핑), 스케일링(표준화/정규화), 범주형 인코딩(one-hot/embedding), 노이즈 제거(필터링, smoothing).\n",
    "    - **EDA(Exploratory Data Analysis)란? 데이터 특성 파악(분포, 상관관계)**\n",
    "        - 목적: 데이터의 분포(히스토그램), 중심·분산(평균·분산), 결측 및 이상치, 변수 간 상관관계(상관행렬, 산점도), 범주별 분포 등을 시각화·요약해 문제 이해와 가설 수립에 사용.\n",
    "    - **회귀에서 절편(intercept)과 기울기(slope)가 의미하는 바는? 딥러닝과의 연관성**\n",
    "        - 선형회귀: y=wx+by = w x + by=wx+b. **기울기(w)**는 x가 한 단위 변할 때 y가 얼마나 변하는지, **절편(b)**은 x=0일 때 예측값.\n",
    "        - 딥러닝에서도 각 층의 **가중치(weight)**와 **편향(bias)**이 같은 역할(선형 변환 + 활성화)로 기능하며, 여러 층이 연결되어 복잡한 함수를 표현.\n",
    "    - **Activation function을 사용하는 이유? Softmax, Sigmoid 함수의 차이**\n",
    "        - 이유: 선형 연산만으로는 여러 층을 쌓아도 전체 모델이 선형이므로 **비선형성**을 넣어 복잡한 패턴을 학습.\n",
    "        - **Sigmoid**: σ(z)=1/(1+e−z)\\sigma(z)=1/(1+e^{-z})σ(z)=1/(1+e−z). 출력 범위 (0,1), 이진확률에 사용되지만 큰 입력에서 기울기 소실(vanishing) 문제.\n",
    "        - **Softmax**: softmax(z)i=ezi/∑jezj\\text{softmax}(z)_i = e^{z_i} / \\sum_j e^{z_j}softmax(z)i=ezi/∑jezj. 다중 클래스 분류에서 클래스별 확률(합 = 1)을 제공. (보통 마지막 층과 categorical cross-entropy 조합)\n",
    "    - **Forward propagation, Backward propagation이란?**\n",
    "        - **Forward**: 입력을 네트워크에 통과시켜 출력과 손실을 계산.\n",
    "        - **Backward**: 체인룰로 손실의 각 파라미터에 대한 기울기(gradient)를 계산하고(역전파), 이를 이용해 파라미터를 업데이트함.\n",
    "    - **손실함수(Loss function)란 무엇인가? 가장 많이 사용하는 손실함수 4가지**\n",
    "        - 정의: 모델 출력과 정답의 차이를 수치화한 함수(목적함수).\n",
    "        - 자주 쓰는 손실:\n",
    "            1. **MSE (Mean Squared Error)** — 회귀.\n",
    "            2. **MAE (Mean Absolute Error)** — 회귀(외란에 덜 민감).\n",
    "            3. **Binary Cross-Entropy** — 이진분류.\n",
    "            4. **Categorical Cross-Entropy** — 다중클래스 분류(softmax와 함께 사용).\n",
    "    - **옵티마이저(optimizer)란 무엇일까? 옵티마이저와 손실함수의 차이점은?**\n",
    "        - **손실함수**: 최적화할 **목적**을 정의(최솟값 찾음).\n",
    "        - **옵티마이저**: 손실을 줄이기 위해 파라미터를 **어떻게 업데이트**할지 결정하는 알고리즘(SGD, Momentum, Adam 등). 즉 목적 vs 최적화 방법의 차이.\n",
    "    - **경사하강법 의미는? (확률적 경사하강법, 배치 경사하강법, 미니배치 경사하강법)**\n",
    "        - **경사하강법(Gradient Descent)**: 손실의 기울기를 따라 파라미터를 갱신하여 손실을 최소화.\n",
    "        - **배치 GD**: 전체 데이터로 한 번에 기울기 계산(정확하지만 느림).\n",
    "        - **확률적 GD (SGD)**: 한 샘플씩 업데이트(빠르지만 노이즈 많음).\n",
    "        - **미니배치 GD**: 소규모 배치(보통 16~512)로 업데이트 — 성능과 안정성의 균형.\n",
    "    - **교차검증, K-fold 교차검증의 의미와 차이**\n",
    "        - **교차검증**: 모델 일반화 성능을 안정적으로 평가하기 위해 데이터를 여러 번 분할해 반복 학습/평가.\n",
    "        - **K-fold**: 데이터를 K개로 나누어 K번 학습(각각 하나를 검증셋으로 사용), 평균 성능을 사용. Stratified K-fold는 클래스 불균형 시 분포 보장.\n",
    "    - **하이퍼파라미터 튜닝이란 무엇인가?**\n",
    "        - 모델이 학습 중에 자동으로 학습되지 않는 설정(예: 학습률, 레이어 수, 정규화 계수 λ, 배치크기)을 **격자 탐색(Grid)**, **랜덤 탐색**, **베이지안 최적화** 등으로 최적화하는 과정.\n",
    "    - **CNN의 합성곱(convolution)의 역할은?**\n",
    "        - 국소 수용 영역(local receptive field)에서 특징(엣지, 패턴)을 추출하고, **가중치 공유**로 파라미터 수를 줄이며 **위치 불변성(translation invariance)**을 제공. 필터는 학습을 통해 패턴을 인식하도록 조정됨.\n",
    "    - **CNN의 풀링층(pooling)의 역할은?**\n",
    "        - 공간 차원(가로·세로)을 **downsample**하여 계산량과 파라미터 감소, 표현의 요약, 작은 위치 변화에 대한 불변성 증대, 과적합 완화(예: max-pooling, average-pooling).\n",
    "    - **CNN의 Dense Layer의 역할은?**\n",
    "        - 합성곱/풀링으로 추출한 특징들을 **전역적으로 결합**해 최종 분류/회귀 출력을 만드는 단계. 보통 네트워크 끝부분에 위치.\n",
    "    - **CNN의 stride, filter의 역할? 필터의 가중치는 어떻게 결정되는가?**\n",
    "        - **Filter(커널)**: 지역 패턴을 감지하는 작은 행렬(예: 3×3).\n",
    "        - **Stride**: 필터를 적용할 때 이동 간격 — stride가 크면 출력 크기 작아짐.\n",
    "        - **가중치 결정**: 초기화 후 **역전파(Backpropagation)**로 손실의 기울기를 통해 업데이트(옵티마이저에 의해 학습).\n",
    "    - **RNN을 사용하는 이유와 한계점은?**\n",
    "        - 이유: 순차 데이터(시계열, 문장 등)의 **시간적 의존성**을 모델링(숨겨진 상태가 과거 정보를 유지).\n",
    "        - 한계: 장기 의존성 학습이 어려움(기울기 소실/폭발), 병렬처리 어려움, 긴 시퀀스에서 비효율.\n",
    "    - **LSTM을 사용하는 이유와 한계점은?**\n",
    "        - 이유: LSTM은 **입력/망각/출력 게이트**를 사용해 장기 의존성을 더 잘 유지(기울기 소실 문제 완화).\n",
    "        - 한계: 구조가 복잡해 계산 비용·파라미터가 큼. 최근에는 Transformer 계열이 많은 시퀀스 문제에서 더 우수하거나 효율적.\n",
    "    - **GRU를 사용하는 이유와 차별성은?**\n",
    "        - GRU는 LSTM을 단순화한 구조(주로 **업데이트·리셋 게이트**)로 파라미터 수가 적고 학습·추론이 빠름. 성능은 데이터셋에 따라 LSTM과 비슷하거나 약간 차이남.\n",
    "    - **결정트리에서 불순도(Impurity) – 지니 계수(Gini Index)란 무엇인가?**\n",
    "        - 지니(Gini) = 1−∑kpk21 - \\sum_{k} p_k^21−∑kpk2, 여기서 pkp_kpk는 노드 내 클래스 k의 비율. 값이 0에 가까울수록 순수한 노드(한 클래스만 존재). 분할 시 지니 감소가 큰 분할을 선택.\n",
    "    - **앙상블이란 무엇인가?**\n",
    "        - 여러 개의 모델(약한 학습기)을 조합해 단일 모델보다 성능·안정성을 개선하는 기법. 대표적으로 **배깅**, **부스팅**, **스태킹** 등이 있음.\n",
    "    - **부트스트랩핑(bootstrapping)이란 무엇인가?**\n",
    "        - 주어진 데이터에서 **복원추출**로 여러 샘플(부트스트랩 데이터셋)을 만드는 방법. 통계적 불확실성 추정이나 배깅에서 사용.\n",
    "    - **배깅(Bagging)이란 무엇인가?**\n",
    "        - Bootstrap Aggregation의 약자. 여러 학습기를 부트스트랩 샘플로 학습시키고 예측을 평균/다수결로 결합해 분산을 줄임(예: 랜덤포레스트는 배깅 + 특성 랜덤선택).\n",
    "    - **주성분 분석(PCA) 이란 무엇인가?**\n",
    "        - 공분산 행렬의 고유벡터(주성분)를 이용해 데이터의 분산을 최대한 보존하는 직교 축으로 투영하는 선형 차원축소 기법. 주로 노이즈 제거, 시각화, 속성 축소에 사용.\n",
    "    - **Dense Layer란 무엇인가?**\n",
    "        - Fully Connected Layer. 입력벡터 xxx에 대해 y=Wx+by = W x + by=Wx+b를 계산한 뒤(필요시 활성화 적용). 입력의 모든 성분과 각 출력 노드가 연결되어 특징을 조합·추상화함.\n",
    "\n",
    "## 머신 러닝 vs 딥러닝\n",
    "\n",
    "- feature를 넣으면 → 머신러닝\n",
    "- row data를 통한 특성 추출+learning ⇒ 딥러닝\n",
    "    - 예) ACC 값을 직접 넣는 것\n",
    "\n",
    "---\n",
    "\n",
    "# 정리\n",
    "\n",
    "> 다음주 수업까지 해야 할 것.\n",
    "> \n",
    "- 깃허브 생성\n",
    "- colab 계정 생성 및 환경 세팅"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
